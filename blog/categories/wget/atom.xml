<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: wget | Beat of The Geek]]></title>
  <link href="http://\.github.io/\/github/blog/categories/wget/atom.xml" rel="self"/>
  <link href="http://\.github.io/\/github/"/>
  <updated>2015-11-04T08:05:54+05:30</updated>
  <id>http://\.github.io/\/github/</id>
  <author>
    <name><![CDATA[Charanjit Singh]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Download in Geek Style: Use Wget (Part 2)]]></title>
    <link href="http://\.github.io/\/github/2011/08/download-in-geek-style-use-wget-part-2.html"/>
    <updated>2011-08-21T00:00:00+05:30</updated>
    <id>http://\.github.io/\/github/2011/08/download-in-geek-style-use-wget-part-2</id>
    <content type="html"><![CDATA[<div class='post'>
<div dir="ltr" style="text-align: left;" trbidi="on">Hello there ! !<br />After our last <a href="http://channikhabra.blogspot.com/2011/08/download-in-geek-style-use-wget-part-1.html">article on Introduction to wget for Linux newbies</a>, it is time to advance a little further. In this article &nbsp;we'll discuss advanced usage of Wget.<br />Let's start with Wget's most wanted command:<br /><br /><span class="Apple-style-span" style="font-size: 19px; font-weight: bold;"><b><span class="Apple-style-span" style="color: #6aa84f; font-size: small;"></span></b></span><br /><b><span class="Apple-style-span" style="color: #6aa84f;">Downloading Recursively (-r switch)</span></b><br /><div><span class="Apple-style-span" style="color: #6aa84f;"><b><br /></b></span>Wget can download recursively, following all the links it meet in the way of downloading process. For example, you are reading an online book (ebook of course), which has links to further chapters. Using this command you can easily download all the pages of the ebook with a single command making your own copy of the ebook to be read offline. Even better, doing some Google we can download as many mp3s or other files as we want, all in a single command.<br />Excited ? (I know you are)<br />All right, enough talking.<br /><span class="Apple-style-span" style="color: #6aa84f; font-weight: bold;"></span><br /><hr /><br /><!-- more --><br /><br /><span class="Apple-style-span" style="color: #6aa84f; font-weight: bold;"><span class="Apple-style-span" style="font-size: large;">How to download recursively</span></span><br /><br /><code> wget -r -l 7 --no-parent&nbsp;</code><span class="Apple-style-span" style="font-family: monospace;">-A pdf,djvu&nbsp;</span><span class="Apple-style-span" style="font-family: monospace;">-nH --cut-dirs=4 -P "My download directory" "Link to download page"&nbsp;</span><br />&nbsp;<b>Time for some explanations:</b><br /><b><span class="Apple-style-span" style="color: #6aa84f;">-r or --recursive</span>&nbsp; &nbsp;</b>This switch tell wget to start downloading recursively from the link given.<br /><br /><b><span class="Apple-style-span" style="color: #6aa84f;">-l or --level='depth'</span></b> &nbsp; &nbsp;When downloading recursively, wget follows a system of levels. This denotes the levels of depth to which wget will follow links.<br />As in above example when we start a download from page 1 with level stated to be 7, wget download main page first. After this it follows all the links given on the page. This is level one. After downloading everything, wget starts following links given in downloaded pages. This is level 2. Similarly wget will download everything &nbsp;following every link it meet in the way until it reaches maximum depth.<br />By default, wget sets depth level to be 5. It can also be set to infinite<br /><blockquote><code>wget -r -l inf "download link" </code> or <code>wget -r l 0 "link"</code></blockquote><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>--no-parent or -np </b></span>&nbsp; Wget's recursive download is bidirectional. It means wget follows link in both directions of link&nbsp;hierarchy (err... what is that?).<br />Let's see an example. Assume we are downloading free ebooks from a website, say example.com/ebooks/english/list.html . So&nbsp;&nbsp;what we want is downloading English books only. But by default, wget will follow all links on list.html BUT it will also move upwards and follow links it find there. This is not what we want.<br />SO here is --no-parent. It is a very useful command which ensures that we download only downwards the hierarchy and don't go upwards. <br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>Download specific file types (-A 'filetype or list' or --accept="list of filetypes")</b></span> &nbsp; &nbsp;When downloading ebooks from our kind website, we don't want any HTML,CSS or Javascript files to be downloaded. By default, wget will download everything including images, scripts &nbsp;and everything. -A or --accept switch allow us to download only desired files. In the example, we want only .pdf and .djvu files to be downloaded, and wget will do that, strictly following our orders. Multiple filetypes can be given separated by commas.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>-R "list of filetypes" or --reject="filetypes"</b></span> &nbsp; Similar to -A is -R. While -A accepts some files and rejects others -R, rejects some given filetypes and download everything else.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b><u>Handling Directories</u></b></span><br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>-P "path" or --directory-prefix="path" </b></span>&nbsp; As stated in <a href="http://channikhabra.blogspot.com/2011/08/download-in-geek-style-use-wget-part-1.html">previous article</a>, -P can be used to redirect downloaded file to some specific path.<br /><br />But when downloading recursively, there is one problem. Wget saves all the files in the same directory hierarchy as they were on the server. As in our example, by default all files will be saved as this,<br /><blockquote>&nbsp;Home Folder&gt; example.com &gt; ebooks &gt; english&gt; file.pdf</blockquote>This behavior can be very irritating for normal users like us. But no worries, wget provides many options to handle this our own way. Here are some most commonly used ones.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>--cut-dirs=x </b></span>&nbsp; &nbsp;This is useful command for controlling the directory structure of the location where recursively downloaded files will be saved. It cut the "x" directory components from the hierarchy.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>--nH or --no-host-directories</b></span> &nbsp; &nbsp;This command cuts the name of host from directory structure. In other words, it disables generation of host prefixed directories.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>-nd or --no-directories</b></span> &nbsp; It suggests wget to not to use any directory structure at all and save all the files in open (by default) or in the folder specified by -P command.<br /><br /><b>Example:</b><br />Assume that we are recursively downloading (pdf) files from example.com/ebooks/english, then this is how they will be saved on our PC with different commands...<br /><br /><blockquote>No options &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;example.com/ebooks/english/file.pdf<br />-nH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ebooks/english/file.pdf<br />-nH --cut-dirs=1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;english/file.pdf<br />--cut-dirs=1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;example.com/english/file.pdf</blockquote>Got it ?....Good... : )<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>Making readable Offline Copies of Websites</b></span><br /><span class="Apple-style-span" style="color: #6aa84f;"><b><br /></b></span>It is really easy to make offline copies of websites, just start a recursive download and it is done. No it is not.<br />&nbsp;Think of the links given on pages. For example, if we download an ebook (HTML files) which has links to next chapters and other such links, all of them point to pages available on server, like &nbsp;an ebook at "example.com/onlinebook/contents" will have link to chapter 1 like this, "example.com/onlinebook/chapter1". Even in an offline copy (that we have made using wget), clicking on this link will take us online to the server. But this is not what we want.<br /><div>Again, no worries, wget has a solution for this.</div><div><br /></div><div><span class="Apple-style-span" style="color: #6aa84f;"><b>-k or --convert-links</b></span>&nbsp; &nbsp; This is an extremely useful option which converts all the links in downloaded pages to their local copies (if they are downloaded). In case HTML files have link to the content which has not been downloaded, wget will convert those links to their absolute location (internet of course). This ensures that there are no broken links and make local viewing smooth.</div><div><br /></div><div><span class="Apple-style-span" style="color: #6aa84f;"><b>-p or --page-requisites </b></span>&nbsp; &nbsp; As all other wget commands, this is also a very useful command which help to download all the files which are necessary for proper display of a page. It downloads everything (images, sounds, style sheet references etc) which are necessary for proper display of page even if they are located on different websites.<br /><br /><b><span class="Apple-style-span" style="color: #6aa84f; font-size: large;">When She said NO !!</span></b><br /><br />Sometimes web servers don't allow tools like wget to access &nbsp;their data and hence we can't download from such servers. But as I am saying from very&nbsp;<a href="http://channikhabra.blogspot.com/2011/08/download-in-geek-style-use-wget-part-1.html">beginning</a>, wget has a way for everything (almost). Here are some useful commands which can be used to get access when the server says no and tempt to kick your ass.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>-U "agent" or --user-agent="agent" </b></span>&nbsp; &nbsp;When wget access a file on a HTTP server, it identifies itself by sending a user agent string (header field). It is like it says to server, "Hey baby, this is wget. Wassup ?". But sometimes HTTP servers deny connections to some agents (web browser, wget etc are all agents which allow us access Internet through protocols) or only allow some specific agents to access their data. We can fool the server by changing the user agent string. The command looks like this:<br /><blockquote><code>wget -U "Mozilla/5.0"</code>&nbsp;or&nbsp;<code>wget --user-agent="Mozilla/5.0"</code></blockquote>Here Mozilla is name of agent and 5.0 is version number. What we are doing is, changing user ID to look as if it was sent by your browser or at least hide the fact that it is sent by wget.<br />Actual User ID string is &nbsp;pretty long and carry more information, but this much is fine for fooling most web servers. We can also say wget to not to send any user ID with this command:<br /><blockquote><code>wget --user-agent=""</code>&nbsp; &nbsp;</blockquote><span class="Apple-style-span" style="color: #6aa84f;"><b>--referer=url </b></span>&nbsp; &nbsp;This command includes the "Referer: url" in the HTTP request. Sometimes servers expect that their data is always accessed by web browsers which are always sent by some page which points to them. This command is not used often, but may be useful in some particular case.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>--http-user=user and --http-password=password</b></span> &nbsp; In case you have an account on the server and server needs username and password to authenticate the request, these commands are to be used. Similar commands are <b>--ftp-user=user</b> and <b>--ftp-password=password</b> for ftp servers and <b>--user=user</b> and <b>--password=password</b> for both http and ftp servers. Latter two have lower preference than first two command sets.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>-w seconds or --wait=seconds </b></span>&nbsp; &nbsp; This command wait for given number of seconds between two consecutive downloads thus decreasing the load on the server. Instead of in seconds, time can be given in minutes with "m" suffix or in hours or even in days with "h" and "d" suffices. Large values can be useful in case destination server is down, giving wget enough time to retry and wait till it is up again.<br /><br /><span class="Apple-style-span" style="color: #6aa84f;"><b>--random-wait </b></span>&nbsp; Sometimes, web servers do analyse the traffic coming to them and find out if automatic tools like wget access them. They usually count the time between requests they&nbsp;receive&nbsp;and deny further requests, --random-wait switch allows us to make wget wait for random time between&nbsp;consecutive&nbsp;downloads and fooling the server.<br />This option causes the&nbsp;time between requests to vary between 0.5 and 1.5 * wait seconds,where wait was specified using the --wait option, in order to mask&nbsp;Wget's presence from such analysis.<br /><div><br /></div><div><span class="Apple-style-span" style="color: #6aa84f;"><b>Unleash the power of Google</b></span><br /><span class="Apple-style-span" style="color: #6aa84f;"><b><br /></b></span></div>Google too has a syntax like *nix commands, which can be used for finding desired results from over billions of pages on Internet. We can get just what we want if we use it smartly. Here we want a pure list of downloadable files, which we can download with wget. Just enter this string in Google Search Bar and hit enter:</div><div><blockquote><i>intitle:"index of/" mp3 "your favorite band" parent directory</i></blockquote>This will give links to pages which only have links to mp3 files of your favorite band ready to download. This link can be passed to wget for a recursive download with required recursion depth to&nbsp;successfully&nbsp;get what we want. Tinker with above search string to get other kind of stuff, may be videos, ebooks or whatever.<br />(This is meant for educational purposes only. Downloading this way is not legal. Use at your own risk... :P)<br /><br />Wget has much more than this. Refer to wget manual pages for more advanced and&nbsp;insight&nbsp;information.<br />HAPPY HACKING... :D<br /><br /><hr style="text-align: justify;" /><div style="text-align: justify;"><br /></div><div style="text-align: justify;"><b><i>Circle Beat Of The Geek on</i></b>&nbsp;<a href="https://plus.google.com/109838896781876000861" target="_blank">Google Plus</a></div><div style="text-align: justify;"><i><b>OR Like us on</b></i>&nbsp;<a href="https://www.facebook.com/pages/Beat-Of-The-Geek/251813454834549" target="_blank">Facebook</a>&nbsp;<b><i>&nbsp;OR Follow on</i></b>&nbsp;<a href="https://twitter.com/#!/beatofthegeek" target="_blank">Twitter</a></div></div></div></div></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Download in Geek Style: Use Wget (Part 1)]]></title>
    <link href="http://\.github.io/\/github/2011/08/download-in-geek-style-use-wget-part-1.html"/>
    <updated>2011-08-06T00:00:00+05:30</updated>
    <id>http://\.github.io/\/github/2011/08/download-in-geek-style-use-wget-part-1</id>
    <content type="html"><![CDATA[<div class='post'>
<div dir="ltr" style="text-align: left;" trbidi="on"><div dir="ltr" style="text-align: left;">I was getting questions about wget from my friends who have recently joined Linux. So, I am writing this article illustrating wget, for an average Linux user, in a little Geeky way.<br /><div>In this article we will discuss several wget commands and split this article into two parts:<br /><blockquote style="text-align: left;"><a href="http://channikhabra.blogspot.com/2011/08/download-in-geek-style-use-wget-part-1.html">Basic Use&nbsp;</a></blockquote><blockquote style="text-align: left;"><a href="http://channikhabra.blogspot.com/2011/08/download-in-geek-style-use-wget-part-2.html">Advanced Use</a></blockquote><div style="text-align: justify;"><br /><br /></div><div style="text-align: justify;"><span style="color: #6aa84f; font-size: large;"><strong>Starting with Wget -- Basic Use:</strong></span><br /><strong><span style="color: #6aa84f;"></span></strong><br /><h3>    <strong><span style="color: #6aa84f;">Starting a Download with wget</span></strong></h3><br />It is easy, just write wget in terminal followed by the url of the file to download (download link)<br /><blockquote><code> wget "download link" </code>(quotes not needed)<br /><!-- more --></blockquote><span style="color: #6aa84f;"><span class="Apple-style-span" style="color: black;"><strong><span style="color: #6aa84f;"></span></strong></span></span><br /><h3>    <span style="color: #6aa84f;"><strong>Downloading Multiple Files</strong></span><strong><span style="color: #6aa84f;"></span></strong></h3><br />Same as above command, just put the download links of all files one after other, followed by a space. Wget assumes everything written after "wget" command to be an URI (unless it starts with "-" sign which is for command switches\mods), and try to download all of them on "first come first serve" basis.<br /><blockquote><code>wget &nbsp;"first link" "second link" "third link" and so on...</code>(quotes not needed)</blockquote><span style="color: #6aa84f;"><span class="Apple-style-span" style="color: black;"><strong><span style="color: #6aa84f;"></span></strong></span></span><br /><h3>    <span style="color: #6aa84f;"><strong>Logging Output</strong></span><strong><span class="Apple-style-span" style="color: #6aa84f;">(-o and -a)</span></strong><strong><span style="color: #6aa84f;"></span></strong></h3><br />When you start downloading with wget, it gives out the output (progress information) to console screen. &nbsp;If you want you can get this output&nbsp;transferred&nbsp;to a specific file.<br /><blockquote><code>wget -o logs.txt "download link"</code></blockquote>If you are continuing an download and don't want the output logs to be rewritten, you can use -a switch. Unlike -o, -a switch appends the output logs into the desired text file. -o rewrites the file every time the command is run.<br /><blockquote><code>wget -a logs "download link"</code></blockquote><strong><span style="color: #6aa84f;"></span></strong><br /><h3>    <span class="Apple-style-span" style="color: #6aa84f;"><strong>Continuing a download (-c or --continue)</strong></span><strong><span style="color: #6aa84f;"></span></strong></h3><br />If a download couldn't get completed when it was started, it can be continued from where it was left (only if server allows continuing downloads). Wget will scan the file in local storage and on the server and will start downloading from where the download ended last time and continue until both files are of equal size (in other words, download is complete).<br /><blockquote><code>wget -c "download link"</code></blockquote>If the server don't allow continuing downloads, wget will refuse the new download and this can ruin the contents of partially downloaded file.<br /><strong><span style="color: #6aa84f;"></span></strong><br /><h3>    <strong><span style="color: #6aa84f;">Downloading a list of files (-i or --input-file)</span></strong><strong><span style="color: #6aa84f;"></span></strong></h3><br />Wget can get a text or even html file as input and will download all the links given in the file with -i switch. For example, if file is a plain text file containing one URI each line, following command is used</div><blockquote><code> wget -i "list of files"</code> (without quotes)</blockquote><div style="text-align: justify;">and if input file is an HTML file containing links to download, -F (--force-html) flag has to be used with above command. e.g.</div><blockquote><code>wget -i "list of files" -F&nbsp;</code>(without quotes)</blockquote><blockquote></blockquote><div style="text-align: justify;"><strong>When downloading multiple files</strong> to the same directory, what wget will do with the new files depends on many things. By default (if no extra switches like -N,-nc,-p or -r are used) wget will re-download the existing files and will &nbsp;rename them as file, file.1, file.2 etc. You can change this behavior to what you want with following switches:</div><div><div style="text-align: justify;"><span style="color: #6aa84f;"><br />Timestamps (-N switch)</span><br />Using time stamps force wget to check the files which are downloaded for their creation time and then comparing it with with the files on the server. If &nbsp;time stamp matches and both files are of same files, wget will skip that file and move on to next file available in the list. <em>&nbsp;</em>If a new version of file is available on the server, then previously downloaded file is overwritten and new file is downloaded. This is very useful when downloading a long list of files.</div><div style="text-align: justify;"><blockquote><code>wget -N "download link or links"</code></blockquote></div><div style="text-align: justify;"><span style="color: #6aa84f;">No Clobbering (-nc or --no-clobber)</span></div><div style="text-align: justify;">When running with -nc or --no-clobber, wget will not download multiple newer copies of the file. Unlike -N switch, it will not download the file if newer versions of the file are available on the server, the previously downloaded version is kept. Remember, it is not clobbering which is prevented with -nc, it is multiple version saving which is prevented.<br /><blockquote><span class="Apple-style-span" style="font-family: monospace;">wget -nc "download link or links"</span></blockquote></div><div style="text-align: justify;"><strong><span style="color: #6aa84f;"></span></strong><br /><h3>    <span class="Apple-style-span" style="color: #6aa84f;"><b><span style="background-color: transparent; color: #6aa84f; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Saving Downloaded Files</span></b></span><strong><span style="color: #6aa84f;"></span></strong></h3><strong><span style="color: #6aa84f;"><br /></span></strong></div><div style="text-align: justify;"><div style="background-color: transparent;"><div dir="ltr" style="color: black; font-weight: normal; margin-bottom: 0pt; margin-top: 0pt; text-align: justify;"><span class="Apple-style-span" style="color: #6aa84f;"><b><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">By default Wget will save all the downloaded files in the same directory from which the terminal is running (usually your home folder). In order to direct the downloaded files to get saved in other location, use "-P" switch</span></b></span></div><div dir="ltr" style="margin-bottom: 0pt; margin-top: 0pt; text-align: justify;"><blockquote style="color: black; font-weight: normal;"><span class="Apple-style-span" style="color: #6aa84f;"><b><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><code> wget -P “path where to save the download” “download link or list”</code></span></b></span></blockquote><br /><h3>    <span class="Apple-style-span" style="color: #6aa84f;"><b>Stop Wget&nbsp;</b></span></h3><h3>    <span class="Apple-style-span" style="font-size: small; font-weight: normal;"><br /></span></h3><h3>    <span class="Apple-style-span" style="font-size: small; font-weight: normal;">When you are running wget from Console (Terminal in other words), simply pressing "ctrl + C " will stop wget. But in case you are running wget in background (with -b switch) or you have chosen to log the output with -o or -a switches, OR you closed the Terminal window by mistake, wget will go in background and can not be closed with ctrl + C.&nbsp;</span></h3>In that case following command will help:<br /><blockquote><code> killall wget </code></blockquote>But this will stop all the instances of wget. In case you are running multiple wget instances, you will first need to find the PID of all wget commands and then close those you want.<br />Use this command to find the PIDs of all the running processes:<br /><blockquote><code>ps -A</code></blockquote>This will show you a list of all the processes running. Find the wget processes and note their PIDs. Then use following command to kill specific process:<br /><blockquote><code>kill 9 "PID"</code> where PID is the Process ID number, without quotes&nbsp;</blockquote>Confusing...huh? <br />Well ! We can make the job easier. Pass following command:<br /><blockquote><code>(ps -A)|grep wget</code></blockquote>and you will see something like this:<br /><br /><blockquote>4860 ? &nbsp; &nbsp; &nbsp; &nbsp;00:00:00 wget</blockquote>4860 is PID of wget command (I have only one instance running at the moment, so only one). Use "kill 9 PID" command and it is all done.<br />Or even easier, use following:<br /><blockquote><code>pgrep wget</code> (for getting PID of wget) OR <code>pkill wget</code> (to instantly stop wget)</blockquote>Easy...HUH...:D<br />&nbsp;This list can go as long as we want. People get confused when they have to close wget running in background, mainly because they don't know how to use command line.<br />But, this is Linux. There are a billion different ways to do one task...BELIEVE IT...: )<br /><br /><div><span class="Apple-style-span" style="color: #6aa84f; font-family: Arial; font-size: 15px; font-weight: bold; white-space: pre-wrap;">Some things you need to know</span></div></div><div dir="ltr" style="margin-bottom: 0pt; margin-top: 0pt; text-align: justify;"><br /><ul><li><span class="Apple-style-span" style="color: #6aa84f; font-family: Arial; font-size: 15px; white-space: pre-wrap;">Wget saves downloaded files in the same directory from where Terminal is running. It is your Home Folder by default. You can change it using "cd" command or with above stated command (-P).</span></li><li><span class="Apple-style-span" style="color: #6aa84f; font-family: Arial;"><span class="Apple-style-span" style="font-size: 15px; white-space: pre-wrap;">Wget will not stop even if you close the Terminal window, it simply goes to background and carry on its job until it is finished. It will not stop even if you log out and switch user without stopping it.</span></span></li><li><span class="Apple-style-span" style="color: #6aa84f; font-family: Arial;"><span class="Apple-style-span" style="font-size: 15px; white-space: pre-wrap;">Wget will not automatically resume previously started downloads, you have to tell it to continue download with "-c" flag.</span></span></li><li><span class="Apple-style-span" style="color: #6aa84f; font-family: Arial;"><span class="Apple-style-span" style="font-size: 15px; white-space: pre-wrap;">Wget has a huge set of commands which when run in right combinations can do variety of jobs, making your life easy. We will look at some of them in later article.</span></span></li></ul><div><span class="Apple-style-span" style="font-family: Arial;"><span class="Apple-style-span" style="white-space: pre-wrap;"><span class="Apple-style-span"></span></span></span><br /><hr \="" /><br /><b><i>Circle Beat Of The Geek on</i></b>&nbsp;<a href="https://plus.google.com/109838896781876000861" target="_blank">Google Plus</a><br /><i><b>OR Like us on</b></i>&nbsp;<a href="https://www.facebook.com/pages/Beat-Of-The-Geek/251813454834549" target="_blank">Facebook</a>&nbsp;<b><i>&nbsp;OR Follow on</i></b>&nbsp;<a href="https://twitter.com/#!/beatofthegeek" target="_blank">Twitter</a><br /></div></div><div style="color: black; font-weight: normal;"></div></div></div></div></div></div></div></div>

]]></content>
  </entry>
  
</feed>
